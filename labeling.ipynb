{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Base URL for NeurIPS proceedings\n",
    "BASE_URL = \"https://papers.nips.cc\"\n",
    "\n",
    "# Headers to mimic a browser\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Specify the year\n",
    "YEAR = 2024  # Change this if needed\n",
    "MAX_PAPERS = 50  # Limit to 150 papers\n",
    "\n",
    "def fetch_paper_links(year):\n",
    "    \"\"\"Fetch paper links from the NeurIPS proceedings page.\"\"\"\n",
    "    url = f\"{BASE_URL}/paper_files/paper/{year}\"\n",
    "    print(f\"Fetching papers from {url}...\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ Failed to retrieve {url}\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Find all paper links\n",
    "        paper_links = [a['href'] for a in soup.find_all('a', href=True) if \"/paper/\" in a['href']]\n",
    "        return paper_links[:MAX_PAPERS]  # Limit to MAX_PAPERS\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"⚠️ Error fetching papers: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_paper_details(paper_link, serial_no):\n",
    "    \"\"\"Extract authors, abstract, and PDF link from a paper page.\"\"\"\n",
    "    paper_url = f\"{BASE_URL}{paper_link}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(paper_url, headers=HEADERS, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ Failed to retrieve {paper_url}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract authors from the <p> tag after <h4> Authors\n",
    "        authors = \"No Authors Available\"\n",
    "        author_heading = soup.find('h4', string=\"Authors\")  # Find h4 with \"Authors\"\n",
    "        if author_heading:\n",
    "            author_paragraph = author_heading.find_next('p')  # Get the next paragraph\n",
    "            if author_paragraph:\n",
    "                italic_authors = author_paragraph.find('i')  # Get authors inside <i> tag\n",
    "                if italic_authors:\n",
    "                    authors = italic_authors.text.strip()\n",
    "\n",
    "        # Extract abstract including those with GitHub links\n",
    "        abstract = \"No Abstract Available\"\n",
    "        abstract_heading = soup.find('h4', string=\"Abstract\")\n",
    "        if abstract_heading:\n",
    "            abstract_paragraph = abstract_heading.find_next('p')\n",
    "            if abstract_paragraph:\n",
    "                abstract = abstract_paragraph.text.strip()\n",
    "                github_link = abstract_paragraph.find('a', href=True)\n",
    "                if github_link:\n",
    "                    abstract += f\" (GitHub: {github_link['href']})\"\n",
    "\n",
    "        # Extract PDF link\n",
    "        pdf_link_tag = soup.find('a', string=\"Paper\")\n",
    "        pdf_link = f\"{BASE_URL}{pdf_link_tag['href']}\" if pdf_link_tag else \"No PDF Available\"\n",
    "\n",
    "        return [serial_no, authors, abstract, pdf_link]\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"⚠️ Error accessing {paper_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to scrape and save NeurIPS papers to a CSV file.\"\"\"\n",
    "    paper_links = fetch_paper_links(YEAR)\n",
    "    \n",
    "    if not paper_links:\n",
    "        print(\"⚠️ No papers found.\")\n",
    "        return\n",
    "\n",
    "    data = []\n",
    "    for index, link in enumerate(paper_links, start=1):\n",
    "        if index > MAX_PAPERS:  # Stop after 150 papers\n",
    "            break\n",
    "        print(f\" Processing Paper {index}/{MAX_PAPERS}...\")\n",
    "        paper_details = extract_paper_details(link, index)\n",
    "        if paper_details:\n",
    "            data.append(paper_details)\n",
    "\n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(data, columns=[\"Serial No\", \"Authors\", \"Abstract\", \"PDF Link\"])\n",
    "    csv_file = f\"neurips_papers_{YEAR}.csv\"\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n",
    "    print(f\"✅ CSV file '{csv_file}' created successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
